{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que las esquinas son características interesantes de una imagen. Los algoritmos de detección de características comenzaron con la detección de esquinas. Hay varias técnicas en OpenCV para detectar las características.\n",
    "<br>\n",
    "Detección de esquinas Haris\n",
    "<br>\n",
    "Detección de esquinas Shi-Tomasi\n",
    "<br>\n",
    "SIFT (Transformación de características de escala invariable)\n",
    "<br>\n",
    "SURF (Funciones robustas aceleradas)\n",
    "<br>\n",
    "Algoritmo FAST para la detección de esquinas\n",
    "<br>\n",
    "ORB (Breve orientado FAST y girado)\n",
    "<br><br>\n",
    "SIFT, SURF están patentados y no están disponibles gratuitamente para uso comercial. Requiere opencv-contrib instalado para poder usarlos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección de esquinas Haris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('images/scene.jpg')\n",
    "\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "gray_img = np.float32(gray_img)\n",
    "\n",
    "dst = cv2.cornerHarris(gray_img, blockSize=2, ksize=3, k=0.04)\n",
    "\n",
    "# dilate to mark the corners\n",
    "dst = cv2.dilate(dst, None)\n",
    "img[dst > 0.01 * dst.max()] = [0, 255, 0]\n",
    "\n",
    "cv2.imshow('haris_corner', img)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección de esquinas Shi-Tomasi\n",
    "Shi y Tomasi idearon una función de puntuación diferente a la utilizada en el detector de esquinas de Haris para encontrar N esquinas más acentuadas a partir de una imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-414cb99b1115>:12: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  cv2.circle(img, (x, y), 6, (0, 255, 0), -1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('images/scene.jpg')\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "corners = cv2.goodFeaturesToTrack(gray_img, maxCorners=50,      qualityLevel=0.02, minDistance=20)\n",
    "corners = np.float32(corners)\n",
    "\n",
    "for item in corners:\n",
    "    x, y = item[0]\n",
    "    cv2.circle(img, (x, y), 6, (0, 255, 0), -1)\n",
    "\n",
    "cv2.imshow('good_features', img)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las dos técnicas anteriores Haris Corner and Shi-Tomasi son invariantes en la rotación, lo que significa que incluso si se giran las esquinas, seremos capaces de detectarlas con exactitud. Sin embargo, son variantes a escala; si se amplían las esquinas perderemos la forma en la región seleccionada y los detectores no podrán identificarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT\n",
    "SIFT es invariante tanto en rotación como en escala. SIFT proporciona puntos clave y descriptores de puntos clave donde el descriptor de puntos clave describe el punto clave en una escala seleccionada y rotación con gradientes de imagen.\n",
    "<br>\n",
    "<br>\n",
    "La imagen resultante tiene círculos que representan los puntos / características clave, donde el tamaño del círculo representa la fuerza del punto clave y la línea dentro del círculo denota la orientación del punto clave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('images/scene.jpg')\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "kp, des = sift.detectAndCompute(gray_img, None)\n",
    "\n",
    "kp_img = cv2.drawKeypoints(img, kp, None, color=(0, 255, 0),                    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "cv2.imshow('SIFT', kp_img)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SURF (Funciones robustas aceleradas)\n",
    "Aunque SIFT funciona bien, realiza operaciones intensivas que requieren mucho tiempo. SURF se introdujo para tener todas las ventajas de SIFT con un tiempo de procesamiento reducido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-wwma2wne\\opencv_contrib\\modules\\xfeatures2d\\src\\surf.cpp:1029: error: (-213:The function/feature is not implemented) This algorithm is patented and is excluded in this configuration; Set OPENCV_ENABLE_NONFREE CMake option and rebuild the library in function 'cv::xfeatures2d::SURF::create'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e08b9f9bf296>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgray_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msurf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxfeatures2d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSURF_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mkp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msurf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-wwma2wne\\opencv_contrib\\modules\\xfeatures2d\\src\\surf.cpp:1029: error: (-213:The function/feature is not implemented) This algorithm is patented and is excluded in this configuration; Set OPENCV_ENABLE_NONFREE CMake option and rebuild the library in function 'cv::xfeatures2d::SURF::create'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('images/scene.jpg')\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "surf = cv2.xfeatures2d.SURF_create()\n",
    "kp, des = surf.detectAndCompute(gray_img, None)\n",
    "\n",
    "kp_img = cv2.drawKeypoints(img, kp, None, color=(0, 255, 0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "cv2.imshow('SURF', kp_img)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo FAST para la detección de esquinas\n",
    "SURF es rápido en comparación con SIFT, pero no tan rápido para usarlo con dispositivos en tiempo real como teléfonos móviles. Así que se introdujo el algoritmo FAST con un tiempo de procesamiento reducido. Sin embargo, FAST nos da solo los puntos clave y es posible que necesitemos calcular descriptores con otros algoritmos como SIFT y SURF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('images/scene.jpg')\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "fast = cv2.FastFeatureDetector_create()\n",
    "fast.setNonmaxSuppression(False)\n",
    "\n",
    "kp = fast.detect(gray_img, None)\n",
    "kp_img = cv2.drawKeypoints(img, kp, None, color=(0, 255, 0))\n",
    "\n",
    "cv2.imshow('FAST', kp_img)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORBE\n",
    "ORB es una alternativa eficiente de código abierto a SIFT y SURF. A pesar de que calcula menos puntos clave en comparación con SIFT y SURF, son efectivos. Utiliza técnicas FAST y BRIEF para detectar los puntos clave y calcular los descriptores de imagen respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('images/scene.jpg')\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "orb = cv2.ORB_create(nfeatures=2000)\n",
    "kp, des = orb.detectAndCompute(gray_img, None)\n",
    "\n",
    "kp_img = cv2.drawKeypoints(img, kp, None, color=(0, 255, 0), flags=0)\n",
    "\n",
    "cv2.imshow('ORB', kp_img)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apareamiento de características\n",
    "La coincidencia de características entre imágenes en OpenCV se puede realizar con un comparador de fuerza bruta o un comparador basado en FLANN.\n",
    "### Comparador de fuerza bruta (BF)\n",
    "BF Matcher compara el descriptor de una característica de una imagen con todas las demás características de otra imagen y devuelve la coincidencia en función de la distancia. Es lento ya que verifica que coincida contra todas las caracterìsticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img1 = cv2.imread('images/book_cover.jpg', 0)\n",
    "img2 = cv2.imread('images/book_cover_rotated.jpg', 0)\n",
    "\n",
    "orb = cv2.ORB_create(nfeatures=500)\n",
    "kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "# matcher takes normType, which is set to cv2.NORM_L2 for SIFT and SURF, cv2.NORM_HAMMING for ORB, FAST and BRIEF\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1, des2)\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "# draw first 50 matches\n",
    "match_img = cv2.drawMatches(img1, kp1, img2, kp2, matches[:50], None)\n",
    "cv2.imshow('Matches', match_img)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLANN based matcher\n",
    "La biblioteca rápida para vecinos más cercanos aproximados (FLANN) está optimizada para encontrar las coincidencias con la búsqueda incluso con grandes conjuntos de datos, por lo que es rápida en comparación con el comparador de fuerza bruta.\n",
    "<br>\n",
    "Con ORB y FLANN matcher extraemos la portada del libro tesla de la segunda imagen y corrijamos la rotación con respecto a la primera imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "im1 = cv2.imread('images/book_cover.jpg', 0)\n",
    "im2 = cv2.imread('images/book_cover_rotated.jpg', 0)\n",
    "\n",
    "img = get_corrected_img(im2, im1)\n",
    "cv2.imshow('Corrected image', img)\n",
    "cv2.waitKey()\n",
    "\n",
    "def get_corrected_img(img1, img2):\n",
    "    MIN_MATCHES = 50\n",
    "\n",
    "    orb = cv2.ORB_create(nfeatures=500)\n",
    "    kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "    kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "    index_params = dict(algorithm=6,\n",
    "                        table_number=6,\n",
    "                        key_size=12,\n",
    "                        multi_probe_level=2)\n",
    "    search_params = {}\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    # As per Lowe's ratio test to filter good matches\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    if len(good_matches) > MIN_MATCHES:\n",
    "        src_points = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_points = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        m, mask = cv2.findHomography(src_points, dst_points, cv2.RANSAC, 5.0)\n",
    "        corrected_img = cv2.warpPerspective(img1, m, (img2.shape[1], img2.shape[0]))\n",
    "\n",
    "        return corrected_img\n",
    "    return img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
